{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how \n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045809bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitting :\n",
    "    when our model is highly trained and it is not give well accuracy in testing data but it give high accuracy in train data\n",
    "    then we can say our model is overfitting. when this type of consequences occure we can say our mode have low bias,\n",
    "    high variance.\n",
    "    \n",
    "consequences:\n",
    "    poor generalization\n",
    "    give bad output (wrong predction made by model on unseen data)\n",
    "    Sensitivity to noise (Overfitting may result in the model picking up random noise, \n",
    "                          leading to unstable and unreliable predictions.)\n",
    "    Reduced interpretability(An overly complex model can become hard to interpret and understand.)\n",
    "\n",
    "mitigation:\n",
    "    Cross-validation\n",
    "    Regularization\n",
    "    Feature selection\n",
    "    Increase the size of the training dataset, which can help the model learn the underlying patterns better\n",
    "    and reduce overfitting.\n",
    "    \n",
    "    \n",
    "underfitting :\n",
    "    when our model give bad accuracy on both dataset, in training dataset and in testing dataset then we can say our model \n",
    "    is underfitting. when this happend we can say it has high bias and high variance.\n",
    "\n",
    "consequences:\n",
    "    Poor performance\n",
    "    Inability to generalize\n",
    "    \n",
    "Mitigation:\n",
    "    Hyperparameter tuning\n",
    "    Complex models\n",
    "    Feature engineering\n",
    "    Increasing the size of the training dataset can sometimes help in reducing underfitting, especially \n",
    "    when the current dataset is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de05ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "there are sevral techniques are used to reduce the overfitting.\n",
    "\n",
    "Cross-validation:\n",
    "    One of the most effective methods to avoid overfitting is cross validation.\n",
    "    This method is different from what we do usually. We use to divide the data in two, cross validation divides \n",
    "    the training data into several sets. The idea is to train the model on all sets except one at each step. \n",
    "    If we have k sets, we will train the model k times with a new testing set at each step. \n",
    "    This cross-validation technique is called k-fold.\n",
    "    \n",
    "Add training data(data augmentation techniques):\n",
    "    Obviously the best solution would be to increase the size of the training data. \n",
    "    Having more samples on the training set, allows the model to be more efficient. \n",
    "    Conversely, if the model is trained with a small amount of data, it is likely to be biased.\n",
    "\n",
    "    Unfortunately, most of the time all our available data is already used. To cope with this we can use data \n",
    "    augmentation techniques.The idea is simple. We make small changes on our training dataset to increase the variety of samples\n",
    "\n",
    "    For example, in computer vision, we have images as training data. We can create filters to slightly modify the colors. \n",
    "    We can rotate the images or stretch lines. This reduces the risk of overfitting.\n",
    "    \n",
    "Remove redundant features:\n",
    "    One of the techniques to improve the performance of a machine learning model is to correctly select the features.\n",
    "    The idea is to remove all features that don’t add any information. If two variables are correlated, \n",
    "    for example, it is better to remove one of them. If a feature has a too low variance, \n",
    "    it doesn’t have any impact on what we are studiying but can distort the results.\n",
    "    In this way, we simplify our data as much as possible, we improve the performance of the model and \n",
    "    we reduce the risk of overfitting.\n",
    "    \n",
    "Regularization methods:\n",
    "    Regularization methods are techniques that reduce the overall complexity of a machine learning model. \n",
    "    They reduce variance and thus reduce the risk of overfitting.\n",
    "    technique:\n",
    "        L1\n",
    "        Ridge\n",
    "        L2\n",
    "        Lasso\n",
    "        \n",
    "Start by designing simple models:\n",
    "    The simpler your model is, the more you will avoid overfitting. The majority of applications can be solved with simple \n",
    "    models.\n",
    "    \n",
    "Early stopping:\n",
    "    Early stopping is a very intuitive technique. It simply consists in stopping the training before it overfits.\n",
    "    This requires finding the optimal training time. To avoid both underfitting and overfitting.\n",
    "    Monitor the models performance on a validation set during training and stop the training process when the performance\n",
    "    starts to degrade. This prevents the model from overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d301135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "underfitting :\n",
    "    when our model give bad accuracy on both dataset, in training dataset and in testing dataset then we can say our model \n",
    "    is underfitting. when this happend we can say it has high bias and high variance.\n",
    "    in underfitting your model is too simple,it means your model is less capabel to capture more pattern from the data.\n",
    "    \n",
    "underfitting occure in model:\n",
    "    use very small dataset or use very less features\n",
    "    we have outliers in our data\n",
    "    Poor Data Quality\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b813663",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and \n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4075fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias is error rate in training dataset. while The difference between the error rate of training data and testing data is called\n",
    "variance. \n",
    "\n",
    "so we can say if our training accuracy is high then it is a low bias, so in order to get low bias we made our mode simple so \n",
    "then if we made our mmodel simple so its test data accuracy is not good. if test dataset accuracy is low then we can say it is \n",
    "a high variance \n",
    "\n",
    "so we try to make model which has a low bias, the model has high variance. this situation is also called as overfitting.\n",
    "\n",
    "in order to make model has low variance means it have good accuracy at testing dataset. if we do this we get\n",
    "model which has high bias means it has low accuracy in training data set.\n",
    "\n",
    "so we can say that if we try to make model which has low bias then we get model which has high variance it is called \n",
    "underfitting.\n",
    "\n",
    "so we can say bias and variance are opposite of each other.\n",
    "any of high value is bad for our model.\n",
    "we have to make both low in order to get best accuracy and performance of the model.\n",
    "\n",
    "so our aim is to make a model which has low variance and low bias. and this type of model is called generalize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. \n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Set Performance:\n",
    "    Evaluate the model on a separate, unseen test set. If the models performance on the test set is much lower than on \n",
    "    the training set, it may be overfitting.\n",
    "    \n",
    "Holdout Validation Set\n",
    "Cross-Validation\n",
    "Learning Curve Analysis\n",
    "Feature Importance Analysis\n",
    "\n",
    "if model give high accuracy in test data but is give low accuracy in testing data then we can say our model is overfitting.\n",
    "if model is not giving good accuracy in both traing and testing dataset then we can say model is underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878afc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias \n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbdf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Bias:\n",
    "- Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "- A high bias model has a strong tendency to underfit the data, meaning it fails to capture the underlying patterns in\n",
    "  the training data.\n",
    "- High bias is often observed when the model is too simple or lacks the complexity to represent the underlying relationships\n",
    "  in the data.\n",
    "- Bias leads to systematic errors, meaning the models predictions are consistently off the mark in the same direction.\n",
    "- Examples of high bias models include linear regression with too few features or a single-layer neural network trying to \n",
    "  solve a complex problem.\n",
    "\n",
    "2. Variance:\n",
    "- Variance refers to the models sensitivity to fluctuations in the training data.\n",
    "- A high variance model has a strong tendency to overfit the data, meaning it performs well on the training data but poorly\n",
    "  on new, unseen data.\n",
    "- High variance is often observed when the model is too complex, and it starts to memorize the noise and random fluctuations\n",
    "  in the training data instead of generalizing.\n",
    "- Variance leads to erratic predictions, as the model becomes highly sensitive to small changes in the input data.\n",
    "- Examples of high variance models include deep neural networks with many layers or decision trees with very deep branches.\n",
    "\n",
    "\n",
    "Comparison:\n",
    "\n",
    "- Bias is related to the error due to the assumptions made by the model, while variance is related to the error due to \n",
    "  the modeds sensitivity to the training data.\n",
    "- High bias leads to underfitting, where the model is too simple to capture the datas complexity. High variance leads to\n",
    "  overfitting, where the model becomes too complex and memorizes noise in the data.\n",
    "- Both high bias and high variance are undesirable because they result in poor generalization to new data. The goal is to \n",
    "  find a balance between the two, achieving a model that neither underfits nor overfits.\n",
    "\n",
    "Trade-off:\n",
    "- The bias-variance trade-off refers to the relationship between bias and variance in machine learning models. \n",
    "  It suggests that as you reduce bias, variance tends to increase, and vice versa.\n",
    "- Finding the right trade-off is essential for building a model that performs well on both the training and test data. \n",
    "  Regularization techniques and cross-validation can help strike this balance.\n",
    "\n",
    "    \n",
    "Example :\n",
    "    if we take the example of student, a student give exams in coaching and he is not able to passing score in coaching exam\n",
    "    and then in final exam he is not able to pass\n",
    "    so while learning he is not able to score we can compare it to the process of training data set. means it is a condition of\n",
    "    high bias. \n",
    "    and then he is not able to score in final exam so it compare to the testing dataset.\n",
    "    means it has high variance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe \n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855741bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in Machine Learning:\n",
    "    Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set\n",
    "    and avoiding overfitting. The commonly used regularization techniques are : \n",
    "        Lasso Regularization – L1 Regularization\n",
    "        Ridge Regularization – L2 Regularization\n",
    "        Elastic Net Regularization – L1 and L2 Regularization\n",
    "        \n",
    "Lasso Regularization – L1 Regularization:\n",
    "    L1 regularization, also known as Lasso, is a regularization technique used in machine learning to prevent overfitting \n",
    "    and improve model performance. It adds a penalty term to the models loss function that is proportional to the absolute\n",
    "    values of the models weights.\n",
    "# (Least Absolute Shrinkage and Selection Operator)\n",
    "\n",
    "Ridge Regularization – L2 Regularization:\n",
    "    L2 regularization adds a penalty term proportional to the square of the models weights. \n",
    "    It forces the weights to be small and penalizes large weights. This smooths the models learning process and\n",
    "    makes it more robust to noise.\n",
    "\n",
    "Elastic Net Regularization – L1 and L2 Regularization:\n",
    "    Elastic Net regularization combines L1 and L2 regularization. It adds both L1 and L2 penalty terms to the loss function.\n",
    "    This provides a balance between feature selection (L1) and weight shrinkage (L2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab2cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
