{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ac8005",
   "metadata": {},
   "source": [
    "## Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3714f3",
   "metadata": {},
   "source": [
    "Feature Selection is the process that removes irrelevant and redundant features from the data set. The model, in turn, will be of reduced complexity, thus, easier to interpret.\n",
    "\n",
    "A subset of features is selected based on their relationship to the target variable. The selection is not dependent of any machine learning algorithm.\n",
    "\n",
    "filter methods are like :\n",
    "-droping constatnt feature or quasi constant\n",
    "-using correlation\n",
    "-anova statistic\n",
    "-chi-square statistic\n",
    "\n",
    "filter method are work very simple:\n",
    "in droping constant feature we just calculate variance of each columns and if the columns variance is 0 we just drop that feature.\n",
    "\n",
    "in same way in using correlation we make some value as a threshold and the columns which have very high correlation we can drop them and handle multicolinearity.\n",
    "\n",
    "we can use anova test when we have feature that are continuous and output feature is categorica with more than 2 category \n",
    "or output feature is numerical we can use anova statistic.\n",
    "\n",
    "we can use chi square test when we have features which are categorical and output feature is also categorical.\n",
    "\n",
    "filter methods are used to select features from set of feature.\n",
    "so we can say filter methods are works on conditions. they are not generating or creating new feature they are just selecting most important feature from set of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1844e",
   "metadata": {},
   "source": [
    "## Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17ff8b",
   "metadata": {},
   "source": [
    "Wrapper and Filter both methods are used for feature selection. And the main diff between this two is Wrapper method\n",
    "use model for selecting feature, when filter method are not requered model for feature selection.\n",
    " \n",
    "in terms of time complexity(computational cost) is high in case of Wrapper methode, in case of Filter method computational \n",
    "cost is less.\n",
    "\n",
    "Filter methods use predefined criteria to rank or score features independently of each other. They do not consider interactions between features.\n",
    "\n",
    "Wrapper methods consider the interaction between features because they evaluate the performance of feature subsets based on a machine learning model's performance. They use techniques like forward selection, backward elimination, or exhaustive search to explore different combinations of features.\n",
    "\n",
    "Filter methods are less prone to overfitting because they do not involve training a model on the data. They only assess the relevance of features based on statistical properties. Filter methods are model-agnostic. They can be used with any machine learning algorithm.\n",
    "\n",
    "Wrapper methods can be more prone to overfitting, especially if the feature selection process is not appropriately regularized or if the dataset is small. They may select features that perform well on the training data but do not generalize well to new data. Wrapper methods are specific to the machine learning algorithm used for evaluation. The choice of the algorithm can impact which features are selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269f49d",
   "metadata": {},
   "source": [
    "## Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8eb488",
   "metadata": {},
   "source": [
    "Embedded feature selection methods are techniques for feature selection that integrate the feature selection process into the training of a machine learning model. These methods aim to select the most relevant features while the model is being built, which can lead to more efficient and accurate models.\n",
    "\n",
    "-Regularaization.\n",
    "-Tree based.\n",
    "\n",
    "L1 Regularization (Lasso Regression):\n",
    "\n",
    "L1 regularization adds a penalty term to the linear regression cost function, encouraging some of the feature coefficients to become exactly zero. As a result, it automatically selects a subset of the most relevant features. Lasso regression is commonly used for feature selection in linear models like linear regression and logistic regression.\n",
    "\n",
    "Tree-Based Methods:\n",
    "\n",
    "Decision Trees and Random Forests: Decision trees and ensemble methods like Random Forests can be used to evaluate feature importance. Features that are frequently used at the top of decision trees or are associated with the largest information gain are considered more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222f577",
   "metadata": {},
   "source": [
    "## Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c71e",
   "metadata": {},
   "source": [
    "Independence Assumption: Filter methods typically evaluate features independently of each other. They don't consider potential interactions or dependencies between features. This can lead to the retention of redundant features or the exclusion of important features that may only contribute when combined with others.\n",
    "\n",
    "Threshold Sensitivity: Filter methods rely on predefined thresholds or scoring metrics to select features. Choosing the right threshold can be challenging and subjective. Different threshold values can lead to significantly different feature subsets and, consequently, varying model performance.\n",
    "\n",
    "Inefficient for High-Dimensional Data: In high-dimensional datasets with many features, filter methods can become computationally expensive as they require evaluating the relevance of each feature independently. This can lead to increased processing time and memory requirements.\n",
    "\n",
    "Information Loss: By selecting features based on a specific scoring metric, filter methods may lead to information loss if some relevant features are pruned, especially if those features contribute to the model in combination with others.\n",
    "\n",
    "Do not remove multicollinearity\n",
    "Sometimes may fail in selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559de35",
   "metadata": {},
   "source": [
    "## Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada75f85",
   "metadata": {},
   "source": [
    "High-Dimensional Data: When dealing with high-dimensional datasets with a large number of features, such as text data or genomic data, the computational cost of the Wrapper method can be prohibitive. Filter methods are computationally efficient and can quickly narrow down the feature set based on statistical properties, making them suitable for initial feature reduction.\n",
    "\n",
    "Quick Feature Selection: If you need a fast and simple way to reduce the number of features to a manageable subset without investing extensive computational resources, the Filter method is a good choice. It provides a quick way to eliminate obviously irrelevant features.\n",
    "\n",
    "Model Agnostic: Filter methods are model-agnostic, meaning you can apply them regardless of the machine learning algorithm you plan to use for your final model. This can be advantageous if you want to explore different algorithms without repeatedly running a wrapper-based feature selection process for each one.\n",
    "\n",
    "Feature Preprocessing: Filter methods can be used as a preprocessing step to reduce the dimensionality of the data before applying more complex modeling techniques. This can be especially helpful when working with algorithms that are sensitive to the curse of dimensionality.\n",
    "\n",
    "Reducing Overfitting: In some cases, you might be more concerned about reducing overfitting rather than optimizing model performance. By removing noisy or irrelevant features with the Filter method, you can make your model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13effd08",
   "metadata": {},
   "source": [
    "## Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several differentones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73931df3",
   "metadata": {},
   "source": [
    "Using the filter methods First we drop the features which have Zero vareance (constatnt Feature).\n",
    "we drop the Duplicate features.\n",
    "we go and calculate the correlation between each feature. \n",
    "Features which are highly corelated we can drop them.\n",
    "and then if there is a still lots of attribute then we can apply chi-square and Anova statistic test. so we can get subset of feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53956b",
   "metadata": {},
   "source": [
    "## Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970dd185",
   "metadata": {},
   "source": [
    "Embedded method work with machine learning models, so which type of model we are choose this is also matter in Embedded method.\n",
    "Configure the selected machine learning algorithm to perform feature selection as part of its training process. How you do this depends on the chosen algorithm:\n",
    "\n",
    "For tree-based models: These models naturally provide feature importances during training. You can rank features by their importance scores and select the top ones.\n",
    "\n",
    "For regularized linear models like Lasso Regression: Adjust the regularization strength (alpha parameter) to control feature sparsity. A higher alpha will lead to more feature selection.\n",
    "\n",
    "For neural networks: Use dropout layers or L1 regularization (e.g., through the Keras library) to encourage feature sparsity during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75e02a",
   "metadata": {},
   "source": [
    "## Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4e119",
   "metadata": {},
   "source": [
    "We already have limited number of feature so we can easly apply wrapper method.\n",
    "we can apply Forword selection and backword selection.\n",
    "\n",
    "Forward Selection: Starts with an empty feature set and iteratively adds one feature at a time, selecting the one that leads to the best model performance.\n",
    "Backward Elimination: Begins with all features and iteratively removes one feature at a time, selecting the one whose removal has the least impact on model performance.\n",
    "\n",
    "so we get best subset of feature by which we get best accuracy.\n",
    "\n",
    "in this case we have features :Size,Location,Age\n",
    "so if we apply forward selection it start will zero feature and then in next itration it take one feature which accuracy is\n",
    "very high and we make new combitation of feature with the selected feature.\n",
    "suppose in first itration we select size\n",
    "so we made combitation like size and location , size and age \n",
    "now from this two which combination have high accuracy we select that combination,assume we get size and location \n",
    "so now in last case we have only one choise to make combination which is age and size and location \n",
    "\n",
    "now from  size , size and location , size and location and age, which combination have large accuracy we just select them.\n",
    "so we select size and location and age.\n",
    "\n",
    "it this type of problem when we have less number of feature we can apply lasso and ridge regression to get more accurate ans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
